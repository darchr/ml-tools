<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Notebooks · ML-Tools</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ML-Tools</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li><a class="toctext" href="../manifest/">Manifest</a></li><li class="current"><a class="toctext" href>Notebooks</a><ul class="internal"><li><a class="toctext" href="#[Basic-Analysis](https://github.com/darchr/ml-notebooks/blob/master/basic_analysis/basic_analysis.ipynb)-1">Basic Analysis</a></li><li><a class="toctext" href="#[ResNet50-in-ImageNet](https://github.com/darchr/ml-notebooks/blob/master/resnet_imagenet/Resnet.ipynb)-1">ResNet50 in ImageNet</a></li><li><a class="toctext" href="#[CPU-Analysis](https://github.com/darchr/ml-notebooks/blob/master/cpu_analysis/cpu_analysis.ipynb)-1">CPU Analysis</a></li><li><a class="toctext" href="#[Batchsize](https://github.com/darchr/ml-notebooks/blob/master/batchsize/batchsizes.ipynb)-1">Batchsize</a></li><li><a class="toctext" href="#[Filters](https://github.com/darchr/ml-notebooks/blob/master/filters/filters.ipynb)-1">Filters</a></li><li><a class="toctext" href="#[Sample-Time](https://github.com/darchr/ml-notebooks/blob/master/sampletime/sampletime.ipynb)-1">Sample Time</a></li></ul></li><li><span class="toctext">Docker</span><ul><li><a class="toctext" href="../docker/docker/">Docker</a></li><li><a class="toctext" href="../docker/tensorflow/">Tensorflow CPU</a></li></ul></li><li><span class="toctext">Datasets</span><ul><li><a class="toctext" href="../datasets/imagenet/">Imagenet</a></li><li><a class="toctext" href="../datasets/slim/">Tensorflow Slim</a></li></ul></li><li><span class="toctext">Workloads</span><ul><li><a class="toctext" href="../workloads/ubuntu/">Ubuntu Workloads</a></li><li><a class="toctext" href="../workloads/tensorflow/">Tensorflow Models</a></li><li><a class="toctext" href="../workloads/slim/">Slim</a></li><li><a class="toctext" href="../workloads/keras/">Keras Models</a></li></ul></li><li><span class="toctext">Launcher</span><ul><li><a class="toctext" href="../launcher/">Launcher</a></li></ul></li><li><span class="toctext">NVM</span><ul><li><a class="toctext" href="../nvm/swap/">Swap</a></li></ul></li><li><span class="toctext">Misc</span><ul><li><a class="toctext" href="../extra/perf/">What the Perf??</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Notebooks</a></li></ul><a class="edit-page" href="https://github.com/darchr/ml-tools/blob/master/docs/src/notebooks.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Notebooks</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Notebooks-1" href="#Notebooks-1">Notebooks</a></h1><p>The notebooks in this repo contain plots and run scripts to generate the data for those  plots. The contents of the notebooks are summarized here and contain links to the rendered notebooks are included.</p><p>In general, each directory contains a notebook and a collection of scripts. Since <code>sudo</code> access is needed to run <code>MemSnoop</code>, these scripts are stand-alone. </p><p>Note that these scripts should be run before the notebooks if trying to recreate the plots.</p><h2><a class="nav-anchor" id="[Basic-Analysis](https://github.com/darchr/ml-notebooks/blob/master/basic_analysis/basic_analysis.ipynb)-1" href="#[Basic-Analysis](https://github.com/darchr/ml-notebooks/blob/master/basic_analysis/basic_analysis.ipynb)-1"><a href="https://github.com/darchr/ml-notebooks/blob/master/basic_analysis/basic_analysis.ipynb">Basic Analysis</a></a></h2><p>Basic analysis of the memory usage during the training of a simple CNN on a single CPU. The sampling window was 0.2 seconds. That is, the sampling procedure went something like this:</p><ol><li>Mark all the applications pages as idle.</li><li>Run application for 0.2 seconds</li><li>Pause application</li><li>Determine which pages are active and update data structures.</li><li>Repeat</li></ol><p>Plots included in this section:</p><ol><li>WSS estimation for a single threaded process.</li><li>Reuse distance analysis.</li><li>Verification that Docker and Python are not interfering with the measurements.</li><li>Heatmap plots visualizing the memory access patterns to the Python heap and for the whole  application during 1 epoch of training.</li></ol><h2><a class="nav-anchor" id="[ResNet50-in-ImageNet](https://github.com/darchr/ml-notebooks/blob/master/resnet_imagenet/Resnet.ipynb)-1" href="#[ResNet50-in-ImageNet](https://github.com/darchr/ml-notebooks/blob/master/resnet_imagenet/Resnet.ipynb)-1"><a href="https://github.com/darchr/ml-notebooks/blob/master/resnet_imagenet/Resnet.ipynb">ResNet50 in ImageNet</a></a></h2><p>Initial look at the memory usage of a large model on ImageNet. This sample data was trained on ResNet50 using a single CPU (yes, very slow). Sampling window was 0.1 seconds.</p><p>The highlight of this is that we can see the forward and backward passes in memory.</p><h2><a class="nav-anchor" id="[CPU-Analysis](https://github.com/darchr/ml-notebooks/blob/master/cpu_analysis/cpu_analysis.ipynb)-1" href="#[CPU-Analysis](https://github.com/darchr/ml-notebooks/blob/master/cpu_analysis/cpu_analysis.ipynb)-1"><a href="https://github.com/darchr/ml-notebooks/blob/master/cpu_analysis/cpu_analysis.ipynb">CPU Analysis</a></a></h2><p>Plots and some analysis of how the memory requirements and training speed for 2 epochs of  training scale as the number of available processors is increased.</p><h2><a class="nav-anchor" id="[Batchsize](https://github.com/darchr/ml-notebooks/blob/master/batchsize/batchsizes.ipynb)-1" href="#[Batchsize](https://github.com/darchr/ml-notebooks/blob/master/batchsize/batchsizes.ipynb)-1"><a href="https://github.com/darchr/ml-notebooks/blob/master/batchsize/batchsizes.ipynb">Batchsize</a></a></h2><p>Data on how WSS and Reuse Distance vary with training batch size. Parameters of experiment:</p><pre><code class="language-none">* Small CNN on Cifar10 dataset
* Single thread
* Unlimited memory
* 0.5 second sampletime
* 1 epoch of training
* Batchsizes: 16, 32, 64, 128, 256, 512, 1024</code></pre><p>I&#39;m not entirely sure what that data means yet ...</p><h2><a class="nav-anchor" id="[Filters](https://github.com/darchr/ml-notebooks/blob/master/filters/filters.ipynb)-1" href="#[Filters](https://github.com/darchr/ml-notebooks/blob/master/filters/filters.ipynb)-1"><a href="https://github.com/darchr/ml-notebooks/blob/master/filters/filters.ipynb">Filters</a></a></h2><p>The goal of this experiment is to see if we can filter out some types of memory during the trace without significantly affecting the results. Filtering out some regions of memory can speed up the idle page tracking process and reduce the memory footprint of the snooper.</p><p>In particular, I explore filtering out Virtual Memory Areas (VMAs) that are</p><pre><code class="language-none">* Executable
* Neither readable nor writable
* Smaller than 4 pages</code></pre><p><strong>Conclusion</strong> - It&#39;s probably okay to do this. However, I need to try this on non single threaded models just in case.</p><h2><a class="nav-anchor" id="[Sample-Time](https://github.com/darchr/ml-notebooks/blob/master/sampletime/sampletime.ipynb)-1" href="#[Sample-Time](https://github.com/darchr/ml-notebooks/blob/master/sampletime/sampletime.ipynb)-1"><a href="https://github.com/darchr/ml-notebooks/blob/master/sampletime/sampletime.ipynb">Sample Time</a></a></h2><p>Experiment to investigate how sensitive our estimates of WSS and Reuse Distance are to the sample time. Parameters of the experiment:</p><pre><code class="language-none">* Small CNN on Cifar dataset
* Both single threaded and with 12 threads
* Sample times of 0.2, 0.5, 1, 2, 4, and 8 seconds
* Batch size of 128
* Training for 1 epoch</code></pre><footer><hr/><a class="previous" href="../manifest/"><span class="direction">Previous</span><span class="title">Manifest</span></a><a class="next" href="../docker/docker/"><span class="direction">Next</span><span class="title">Docker</span></a></footer></article></body></html>
