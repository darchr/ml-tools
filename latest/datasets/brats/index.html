<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>BraTS · ML-Tools</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ML-Tools</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><span class="toctext">Launcher</span><ul><li><a class="toctext" href="../../launcher/launcher/">Tutorial</a></li><li><a class="toctext" href="../../launcher/docstrings/">Docstrings</a></li></ul></li><li><span class="toctext">Workloads</span><ul><li><a class="toctext" href="../../workloads/tf_benchmarks/">Tensorflow Benchmarks</a></li><li><a class="toctext" href="../../workloads/ngraph/">NGraph Models</a></li><li><a class="toctext" href="../../workloads/cifarcnn/">Cifar Cnn</a></li><li><a class="toctext" href="../../workloads/test/">Test Workload</a></li><li><a class="toctext" href="../../workloads/slim/">Tensorflow Slim</a></li></ul></li><li><span class="toctext">Datasets</span><ul><li><a class="toctext" href="../imagenet/">Imagenet</a></li><li><a class="toctext" href="../rnn/">RNN Translator</a></li><li class="current"><a class="toctext" href>BraTS</a><ul class="internal"><li><a class="toctext" href="#Preprocessing-1">Preprocessing</a></li><li><a class="toctext" href="#Problems-Solutions-1">Problems + Solutions</a></li></ul></li></ul></li><li><a class="toctext" href="../../manifest/">Manifest</a></li><li><span class="toctext">NVM</span><ul><li><a class="toctext" href="../../nvm/swap/">Swap</a></li></ul></li><li><span class="toctext">Misc</span><ul><li><a class="toctext" href="../../extra/perf/">PAPI Notes</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Datasets</li><li><a href>BraTS</a></li></ul><a class="edit-page" href="https://github.com/darchr/ml-tools/blob/master/docs/src/datasets/brats.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>BraTS</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="BraTS-1" href="#BraTS-1">BraTS</a></h1><p>Brain Tumor Segmentation library. Specifically, the 2018 edition. Getting this dataset is kind of a pain because you have to register, and then the people hosting the registration don&#39;t actually tell you when your registration is ready.</p><p>More information can be found at <a href="https://www.med.upenn.edu/sbia/brats2018/data.html">https://www.med.upenn.edu/sbia/brats2018/data.html</a></p><p>Once you have the <code>zip</code> file of the data, titled <code>MICCAI_BraTS_2018_Data_Training.zip</code>,  getting it into a format that is useable by the 3dUnetCNN workload is pretty involved:</p><h2><a class="nav-anchor" id="Preprocessing-1" href="#Preprocessing-1">Preprocessing</a></h2><p>Create a directory where the dataset will go, and make a folder called &quot;original&quot; in it.</p><pre><code class="language-none">mkdir ~/brats
cd brats
mkdir original
cd original</code></pre><p>Move the zip file into the <code>original</code> folder</p><pre><code class="language-none">mv &lt;zip-path&gt; .</code></pre><p>Unzip the contents of the file</p><pre><code class="language-none">unzip `MICCAI_BraTS_2018_Data_Training.zip`</code></pre><p>Now, go build the docker container <code>darchr/3dunet</code> (see the 3dunet page). Once that is done, run the <code>preprocess.sh</code> script in <code>workloads/3dUnet/dataset/preprocess.sh</code> using</p><pre><code class="language-none">./preprocess.sh ~/brats</code></pre><p>Go have a snack while this thing runs. I&#39;m sorry if you don&#39;t have a machine with 96  processors because it will take a while.</p><p>Once the preprocess script is done, there&#39;s still more preprocessing to do. Unfortunately, factoring out the code that runs this step proved to be more challenging than I was willing to deal with, so you will have to run this workload. Basically, the first step that the  3dUnet implementation does is to turn all the preprocessed files into a gigantic hdf5 file. But, it only has to do it once.</p><p>Make sure you register the location of the <code>brats</code> data repo in Launcher with</p><pre><code class="language-none">cd Launcher

julia</code></pre><pre><code class="language-julia">julia&gt; using Launcher

julia&gt; Launcher.edit_setup()</code></pre><p>Then, run the workload with</p><pre><code class="language-julia">julia&gt; workload = Launcher.Unet()

julia&gt; run(workload)</code></pre><p>Wait patiently the initial conversion to hdf5 to complete. Once it does, you&#39;ll never have to deal with this stuff again (hopefully).</p><h2><a class="nav-anchor" id="Problems-Solutions-1" href="#Problems-Solutions-1">Problems + Solutions</a></h2><ul><li><p>The python:3.5.6 docker container had a operating system that was to old for the compilers/   cmake versions to build ANTs. Thus, I switched <code>darchr/tensorflow-mkl</code> to be based on   ubuntu 18.04 and build python 3.5.6 from source in that container.</p></li><li><p>When building ANTs, the make process would just hang when trying to download TKv5 (or    something with a name very similar to that). The problem was with the git protocol used   to clone the repository. The solution to this was to pass a flag to cmake:</p></li></ul><pre><code class="language-none">cmake -DSuperBuild_ANTS_USE_GIT_PROTOCOL=OFF ../ANTs</code></pre><ul><li><p>The 3dUnet implementation, especially the data loading from the HDF5 file is insanely    buggy - it would immediately segfault then loading data. My solutions to this, taken    from comments of users in issues for the repository, was to</p><ul><li><p>Turn off compresion into the HDF5 file in <code>data.py</code>, line 12: change the key word    arguments to just <code>complevel=0</code></p></li><li><p>Enable multithreading in the training loop <code>training.py</code>: add the argument   <code>use_multiprocessing = True</code> to the fall to <code>fit_generator</code> on line 78.</p></li></ul></li></ul><footer><hr/><a class="previous" href="../rnn/"><span class="direction">Previous</span><span class="title">RNN Translator</span></a><a class="next" href="../../manifest/"><span class="direction">Next</span><span class="title">Manifest</span></a></footer></article></body></html>
