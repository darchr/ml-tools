<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>VGG416/Slim · ML-Tools</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>ML-Tools</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../manifest/">Manifest</a></li><li><a class="toctext" href="../../notebooks/">Notebooks</a></li><li><span class="toctext">Workloads</span><ul><li><a class="toctext" href="../primary/">Primary Workloads</a></li><li><a class="toctext" href="../ubuntu/">Ubuntu Workloads</a></li><li class="current"><a class="toctext" href>VGG416/Slim</a><ul class="internal"><li><a class="toctext" href="#Using-from-Launcher-1">Using from Launcher</a></li><li><a class="toctext" href="#Dataset-1">Dataset</a></li><li><a class="toctext" href="#Docker-Tensorflow-CPU-1">Docker - Tensorflow CPU</a></li><li><a class="toctext" href="#Script-Arguments:-1">Script Arguments:</a></li><li><a class="toctext" href="#File-Changes-1">File Changes</a></li></ul></li><li><a class="toctext" href="../keras/">Keras Models</a></li></ul></li><li><span class="toctext">Launcher</span><ul><li><a class="toctext" href="../../launcher/">Launcher</a></li></ul></li><li><span class="toctext">NVM</span><ul><li><a class="toctext" href="../../nvm/swap/">Swap</a></li></ul></li><li><span class="toctext">Misc</span><ul><li><a class="toctext" href="../../extra/perf/">PAPI Notes</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Workloads</li><li><a href>VGG416/Slim</a></li></ul><a class="edit-page" href="https://github.com/darchr/ml-tools/blob/master/docs/src/workloads/slim.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>VGG416/Slim</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="VGG416/Slim-1" href="#VGG416/Slim-1">VGG416/Slim</a></h1><p>This is actually a collection of models implemented using using Tensorflow&#39;s Slim framework. The original repo for these models is  <a href="https://github.com/tensorflow/models/tree/master/research/slim">https://github.com/tensorflow/models/tree/master/research/slim</a>.</p><p>When I benchmarked this against the official tensorflow models for Resnet, this  implementation seemed to train a little faster. Plus, the official models did not have VGG implemented, which is why I ended up using this implementation.</p><h2><a class="nav-anchor" id="Using-from-Launcher-1" href="#Using-from-Launcher-1">Using from Launcher</a></h2><h2><a class="nav-anchor" id="Dataset-1" href="#Dataset-1">Dataset</a></h2><p>This collection of models uses the Imagenet dataset.</p><h3><a class="nav-anchor" id="Preparation-steps-(don&#39;t-need-to-repeat)-1" href="#Preparation-steps-(don&#39;t-need-to-repeat)-1">Preparation steps (don&#39;t need to repeat)</a></h3><p>The code in this repo is taken from the build process that comes in the <code>slim</code> project. However, I&#39;ve modified it so it works without having to go through Bazel (I don&#39;t really know why that was used in the first place) and also updated it so it works with Python3.</p><p><strong>Changes made to build</strong></p><ul><li><p><code>download_and_convert_imagenet.sh</code></p><ul><li><p>Removed some build comments that are no longer relevant.</p></li><li><p>Line 59: Change path for <code>WORK_DIR</code> since we&#39;re no longer doing the Bazel style   build.</p></li><li><p>Line 104: Change path to <code>build_iamgenet_data.py</code>.</p></li><li><p>Line 108: Put <code>python3</code> in front of script invocation. Get around executable   permission errors.</p></li></ul></li><li><p><code>datasets/build_imagenet_data.py</code></p><ul><li><p>Lines 213, 216, 217, and 224: Suffix <code>.encode()</code> on string arguments to pass them   as bytes to <code>_bytes_feature</code>.</p></li><li><p>Lines 527: Wrap <code>range(len(filenames))</code> in <code>list()</code> to materialize the lazy range   type.</p></li></ul></li><li><p><code>datasets/download_imagenet.sh</code></p><ul><li>Lines 72 and 81: Comment out <code>wget</code> commands, avoid downloading imagenet training   and validation data.</li></ul></li><li><p><code>datasets/preprocess_imagenet_validation_data.py</code></p><ul><li><p>Line 1: <code>#!/usr/bin/python</code> -&gt; <code>#!/usr/bin/python3</code></p></li><li><p>Remove importing of <code>six.moves</code> module.</p></li><li><p>Change all instances of <code>xrange</code> to <code>range</code>. The <code>range</code> type in python3 behaves   just like the xrange type.</p></li></ul></li><li><p><code>datasets/process_bounding_boxes.py</code></p><ul><li><p>Line 1: <code>#!/usr/bin/python</code> -&gt; <code>#!/usr/bin/python3</code></p></li><li><p>Remove importing of <code>six.moves</code> module.</p></li><li><p>Change all instance of <code>xrange</code> to <code>range</code>.</p></li></ul></li></ul><h3><a class="nav-anchor" id="Steps-for-building-slim-1" href="#Steps-for-building-slim-1">Steps for building <code>slim</code></a></h3><ul><li>Put <code>ILSVRC2012_img_train.tar</code> and <code>ILSVRC2012_img_val.tar</code> in a known spot </li></ul><p>(<code>&lt;path/to/imagenet&gt;</code>) with 500GB+ of available memory.</p><p>Navigate in this repository to: <code>/datasets/imagenet/slim</code>. Launch a Tensorflow docker container with</p><pre><code class="language-sh">docker run -it --rm \
    -v &lt;path/to/imagnet&gt;:/imagenet \
    -v $PWD:/slim-builder \
    -e LOCAL_USER_ID=$UID \
    darchr/tf-compiled-base /bin/bash</code></pre><p>inside the docker container, run:</p><pre><code class="language-sh">cd slim-builder
$PWD/download_and_convert_imagenet.sh /imagenet</code></pre><p>When prompted to enter in your credentials, just hit enter. The script won&#39;t download imagenet anyways so it doesn&#39;t matter what you put in.  Hopefully, everything works  as expected. If not, you can always edit the <code>download_and_convert_imagenet.sh</code> file,  commenting out the script/python invokations that have already completed.</p><h2><a class="nav-anchor" id="Docker-Tensorflow-CPU-1" href="#Docker-Tensorflow-CPU-1">Docker - Tensorflow CPU</a></h2><p>The Docker Hub where the most current version of this container lives is here: <a href="https://hub.docker.com/r/darchr/tf-compiled-base/">https://hub.docker.com/r/darchr/tf-compiled-base/</a>. This repo will be kept  up-to-date as I make needed changes to the container.</p><p>I&#39;m using the official tensorflow docker approach to compile and build the pip package for tensor flow.</p><ul><li><a href="https://www.tensorflow.org/install/source">https://www.tensorflow.org/install/source</a></li><li><a href="https://www.tensorflow.org/install/docker">https://www.tensorflow.org/install/docker</a></li></ul><p>Helpful post talking about docker permissions <a href="https://denibertovic.com/posts/handling-permissions-with-docker-volumes/">https://denibertovic.com/posts/handling-permissions-with-docker-volumes/</a></p><h3><a class="nav-anchor" id="Compilation-Overview-1" href="#Compilation-Overview-1">Compilation Overview</a></h3><p>Containers will be build incrementally, starting with <code>darchr/tf-compiled-base</code>, which is the base image containing Tensorflow that has been compiled on <code>amarillo</code>. Compiling Tensorflow is important because the default Tensorflow binary is not compiled to use AVX2 instructions. Using the very scientific &quot;eyeballing&quot; approach, this compiled version of Tensorflow runs ~60% faster.</p><p>Other containers that use Tensorflow can be build from <code>darchr/tf-compiled/base</code>.</p><h3><a class="nav-anchor" id="darchr/tf-compiled-base-1" href="#darchr/tf-compiled-base-1"><code>darchr/tf-compiled-base</code></a></h3><p>As a high level overview, we use an official Tensorflow docker containers to build a  Python 3.5 &quot;wheel&quot; (package). We then use a Python 3.5.6 docker container as a base to  install the compiled tensorflow wheel.</p><h4><a class="nav-anchor" id="Compiling-Tensorflow-1" href="#Compiling-Tensorflow-1">Compiling Tensorflow</a></h4><p>Pull the docker container with the source code:</p><pre><code class="language-sh">docker pull tensorflow/tensorflow:1.12.0-devel-py3</code></pre><p>Launch the container with</p><pre><code class="language-sh">docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=&quot;$(id -u):$(id -g)&quot; tensorflow/tensorflow:1.12.0-devel-py3 bash</code></pre><p>This does the following:</p><ul><li><p>Opens the container in the <code>/tensorflow</code> directory, which contains the tensorflow source   code</p></li><li><p>Mounts the current directory into the <code>/mnt</code> directory in the container. This allows the   .whl build to be dropped in the PWD after compilation.</p></li></ul><p>Inside the container, run</p><pre><code class="language-sh">git pull</code></pre><p>to pull the latest copy of the tensorflow source. Then configure the build with</p><pre><code class="language-sh">./configure</code></pre><p>Settings used:</p><ul><li>Python Location: default</li><li>Python Library Path: default</li><li>Apache Ignite Support: Y</li><li>XLA Jit support: Y</li><li>OpenCL SYCL support: N</li><li>ROCm support: N</li><li>CUDA support: N</li><li>Fresh clang release: N</li><li>MPI support: N</li><li>Optimization flags: default</li><li>Interactively configure ./WORKSPACE: N</li></ul><p>Steps to build:</p><pre><code class="language-none">bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /mnt
chown $HOST_PERMS /mnt/tensorflow-1.12.1-cp35-cp35m-linux_x86_64.whl</code></pre><p>Note, compilation takes quite a while, so be patient. If running on amarillo, enjoy the 96 thread awesomeness.</p><h4><a class="nav-anchor" id="Summary-1" href="#Summary-1">Summary</a></h4><pre><code class="language-sh">docker pull tensorflow/tensorflow:nightly-devel-py3
docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS=&quot;$(id -u):$(id -g)&quot; tensorflow/tensorflow:nightly-devel-py3 bash
# inside container
git pull
./configure # Look at options above
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /mnt
chown $HOST_PERMS /mnt/tensorflow-1.12.1-cp35-cp35m-linux_x86_64.whl</code></pre><h4><a class="nav-anchor" id="Building-the-Docker-Image-1" href="#Building-the-Docker-Image-1">Building the Docker Image</a></h4><p>With the <code>.whl</code> for tensorflow build, we can build a new Docker container with this  installed. For this step, move <code>tensorflow-...-.whl</code> into the <code>tf-compiled-base/</code>  directory. Then, run the shell script:</p><pre><code class="language-sh">./build.sh tensorflow-1.12.1-cp35-cm35m-linux_x86_64.whl</code></pre><p>Finally, if necessary, push the image to the <code>darchr</code> docker hub via</p><pre><code class="language-sh">docker push darchr/tf-compiled-base</code></pre><h4><a class="nav-anchor" id="Some-Notes-1" href="#Some-Notes-1">Some Notes</a></h4><p>Annoyingly, the <code>.whl</code> created in the previous step only works with Python 3.5. I tried  hacking it by changing the name (cp35-cp35m -&gt; cp36-cp36m), but installation with <code>pip</code>  failed. This means that we need a working copy of Python 3.5 in order to run this.  Fortunately, the Python foundation supplies Debian (I think ... or Ubuntu) based containers for past Python versions. We can use this as a starting point for our <code>Dockerfile</code>.</p><p>Permissions with the docker containers was becoming a bit of a nightmare. I finally found a solution that works by installing <code>gosu</code>:</p><ul><li><a href="https://github.com/tianon/gosu">https://github.com/tianon/gosu</a></li><li><a href="https://denibertovic.com/posts/handling-permissions-with-docker-volumes/">https://denibertovic.com/posts/handling-permissions-with-docker-volumes/</a></li></ul><p>Essentially, a dummy account <code>user</code> is created that does not have root privileges, but we can still create directories within the docker containers.</p><h2><a class="nav-anchor" id="Script-Arguments:-1" href="#Script-Arguments:-1">Script Arguments:</a></h2><pre><code class="language-none">Generic training script that trains a model using a given dataset.
flags:

/models/slim/train_image_classifier.py:
  --adadelta_rho: The decay rate for adadelta.
    (default: &#39;0.95&#39;)
    (a number)
  --adagrad_initial_accumulator_value: Starting value for the AdaGrad accumulators.
    (default: &#39;0.1&#39;)
    (a number)
  --adam_beta1: The exponential decay rate for the 1st moment estimates.
    (default: &#39;0.9&#39;)
    (a number)
  --adam_beta2: The exponential decay rate for the 2nd moment estimates.
    (default: &#39;0.999&#39;)
    (a number)
  --batch_size: The number of samples in each batch.
    (default: &#39;32&#39;)
    (an integer)
  --checkpoint_exclude_scopes: Comma-separated list of scopes of variables to exclude when restoring from a checkpoint.
  --checkpoint_path: The path to a checkpoint from which to fine-tune.
  --[no]clone_on_cpu: Use CPUs to deploy clones.
    (default: &#39;false&#39;)
  --dataset_dir: The directory where the dataset files are stored.
  --dataset_name: The name of the dataset to load.
    (default: &#39;imagenet&#39;)
  --dataset_split_name: The name of the train/test split.
    (default: &#39;train&#39;)
  --end_learning_rate: The minimal end learning rate used by a polynomial decay learning rate.
    (default: &#39;0.0001&#39;)
    (a number)
  --ftrl_initial_accumulator_value: Starting value for the FTRL accumulators.
    (default: &#39;0.1&#39;)
    (a number)
  --ftrl_l1: The FTRL l1 regularization strength.
    (default: &#39;0.0&#39;)
    (a number)
  --ftrl_l2: The FTRL l2 regularization strength.
    (default: &#39;0.0&#39;)
    (a number)
  --ftrl_learning_rate_power: The learning rate power.
    (default: &#39;-0.5&#39;)
    (a number)
  --[no]ignore_missing_vars: When restoring a checkpoint would ignore missing variables.
    (default: &#39;false&#39;)
  --label_smoothing: The amount of label smoothing.
    (default: &#39;0.0&#39;)
    (a number)
  --labels_offset: An offset for the labels in the dataset. This flag is primarily used to evaluate the VGG and ResNet architectures which do not use a background class for the ImageNet
    dataset.
    (default: &#39;0&#39;)
    (an integer)
  --learning_rate: Initial learning rate.
    (default: &#39;0.01&#39;)
    (a number)
  --learning_rate_decay_factor: Learning rate decay factor.
    (default: &#39;0.94&#39;)
    (a number)
  --learning_rate_decay_type: Specifies how the learning rate is decayed. One of &quot;fixed&quot;, &quot;exponential&quot;, or &quot;polynomial&quot;
    (default: &#39;exponential&#39;)
  --log_every_n_steps: The frequency with which logs are print.
    (default: &#39;10&#39;)
    (an integer)
  --master: The address of the TensorFlow master to use.
    (default: &#39;&#39;)
  --max_number_of_steps: The maximum number of training steps.
    (an integer)
  --model_name: The name of the architecture to train.
    (default: &#39;inception_v3&#39;)
  --momentum: The momentum for the MomentumOptimizer and RMSPropOptimizer.
    (default: &#39;0.9&#39;)
    (a number)
  --moving_average_decay: The decay to use for the moving average.If left as None, then moving averages are not used.
    (a number)
  --num_clones: Number of model clones to deploy. Note For historical reasons loss from all clones averaged out and learning rate decay happen per clone epochs
    (default: &#39;1&#39;)
    (an integer)
  --num_epochs_per_decay: Number of epochs after which learning rate decays. Note: this flag counts epochs per clone but aggregates per sync replicas. So 1.0 means that each clone will go
    over full epoch individually, but replicas will go once across all replicas.
    (default: &#39;2.0&#39;)
    (a number)
  --num_preprocessing_threads: The number of threads used to create the batches.
    (default: &#39;4&#39;)
    (an integer)
  --num_ps_tasks: The number of parameter servers. If the value is 0, then the parameters are handled locally by the worker.
    (default: &#39;0&#39;)
    (an integer)
  --num_readers: The number of parallel readers that read data from the dataset.
    (default: &#39;4&#39;)
    (an integer)
  --opt_epsilon: Epsilon term for the optimizer.
    (default: &#39;1.0&#39;)
    (a number)
  --optimizer: The name of the optimizer, one of &quot;adadelta&quot;, &quot;adagrad&quot;, &quot;adam&quot;,&quot;ftrl&quot;, &quot;momentum&quot;, &quot;sgd&quot; or &quot;rmsprop&quot;.
    (default: &#39;rmsprop&#39;)
  --preprocessing_name: The name of the preprocessing to use. If left as `None`, then the model_name flag is used.
  --quantize_delay: Number of steps to start quantized training. Set to -1 would disable quantized training.
    (default: &#39;-1&#39;)
    (an integer)
  --replicas_to_aggregate: The Number of gradients to collect before updating params.
    (default: &#39;1&#39;)
    (an integer)
  --rmsprop_decay: Decay term for RMSProp.
    (default: &#39;0.9&#39;)
    (a number)
  --rmsprop_momentum: Momentum.
    (default: &#39;0.9&#39;)
    (a number)
  --save_interval_secs: The frequency with which the model is saved, in seconds.
    (default: &#39;600&#39;)
    (an integer)
  --save_summaries_secs: The frequency with which summaries are saved, in seconds.
    (default: &#39;600&#39;)
    (an integer)
  --[no]sync_replicas: Whether or not to synchronize the replicas during training.
    (default: &#39;false&#39;)
  --task: Task id of the replica running the training.
    (default: &#39;0&#39;)
    (an integer)
  --train_dir: Directory where checkpoints and event logs are written to.
    (default: &#39;/tmp/tfmodel/&#39;)
  --train_image_size: Train image size
    (an integer)
  --trainable_scopes: Comma-separated list of scopes to filter the set of variables to train.By default, None would train all the variables.
  --weight_decay: The weight decay on the model weights.
    (default: &#39;4e-05&#39;)
    (a number)
  --worker_replicas: Number of worker replicas.
    (default: &#39;1&#39;)
    (an integer)</code></pre><h2><a class="nav-anchor" id="File-Changes-1" href="#File-Changes-1">File Changes</a></h2><p><strong><code>train_image_classifier.py</code></strong></p><ul><li>Line 62: Change default value of <code>log_every_n_steps</code> from 10 to 5.</li></ul><footer><hr/><a class="previous" href="../ubuntu/"><span class="direction">Previous</span><span class="title">Ubuntu Workloads</span></a><a class="next" href="../keras/"><span class="direction">Next</span><span class="title">Keras Models</span></a></footer></article></body></html>
