#####
##### Initial Setup
#####

# For just the initial formulation, we need the following pipeline:
#
# - Base data structure will be a vector of nGraph ops in `ordered_ops` structure,
#   which is the order that they are executed by the nGraph runtime.
#
# - Get a list of all intermediate tensors and sizes.
#   Each tensor can either live in DRAM or PMEM, so we need to generate JuMP variables
#   accordingly.
#
# - Do a liveness analysis to determine where tensors begin and when they go out of
#   scope.
#
# - Iterate through each op in the nGraph ops. For each op, generate
#
#   1. A capacity constraint on the number of live tensors in DRAM.
#   2. Generate a `gadget` to encode the running time of the kernel given the locations
#      of its inputs and outputs.
#   3. Add the results of this gadget to the global objective function, which will be
#      to minimize the sum of active running times.
#
# To accomplish this, we need to pass around a JuMP `Model` which we may progressively
# add variables and constraints to.
#
# To sequentially build the objective function, we can have a JuMP `expression`
# (http://www.juliaopt.org/JuMP.jl/v0.19.0/expressions/) which we update with the
# `add_to_expression!` function at each node in the graph.

# Struct to be passed around since all these items are generally used together anyways.
mutable struct Frame{T}
    modeltype::T
    model::JuMP.Model
    profile_data::ProfileData
end

limit(F::Frame) = limit(F.modeltype)

JuMP.optimize!(F::Frame) = optimize!(F.model)

include("affinity.jl")
include("ilp.jl")
include("inspect.jl")
include("configure.jl")
include("modnn/modnn.jl")
include("numa/numa.jl")

function actualize(backend, func; nkw...)
    f, args, kw = func()
    return nGraph.compile(backend, f, args...; kw..., nkw...)
end

# Absolute optimizers just fall through to `_factory`
function factory(
        backend::nGraph.Backend,
        func,
        opt::AbstractOptimizer{Int64}
    )

    return _factory(backend, func, opt)
end

# Ratio optimizers go through a refinement step
function factory(
        backend::nGraph.Backend{nGraph.CPU}, 
        func, 
        opt::AbstractOptimizer{Rational{Int64}}; 
        search_ratio = true,
        refinements = 3
    )

    @info "Trying Ratio $(getratio(opt))"

    # Just return the inner factory if we aren't interesting in performing a binary search
    # for the actual ratio to input that will return the desired ratio
    search_ratio || return _factory(backend, func, opt)

    # Perform a binary search
    ret = _factory(backend, func, opt)   
    fex = first(ret)
    args = Base.tail(ret)

    # If we're within the desired tolerance, just return like normal
    checkmargin(fex, opt) && return (fex, args...)

    # For now, I'm assuming that the ratio generated by the optimized graph will always
    # be greater than the desired ratio due to defragmentation.
    @assert getratio(fex) > getratio(opt)

    # Start doing a grid search
    #
    # The function desired_ratio -> actual_ratio is not necessarily monotonic, so we can't
    # use a binary search. Instead this grid search thing seems to work alright.
    best_ratio = getratio(opt)
    best_err = geterr(fex, opt)
    current_ratio = best_ratio

    best_fex = fex
    best_args = args

    for i in 1:refinements
        # Use a step size starting with 1 and increasing or decreasing by the step size 
        # until the ratio crosses the boundary of what we want.
        step = 1 // (10 ^ (i - 1))
        @info """
        ------------------------
        Performing Refinement Iteration $i
        Step: $step
        ------------------------
        """

        for _ in 1:9
            current_ratio -= step
            @info "Trying Ratio: $(convert(Float64, current_ratio))"
            ret = _factory(backend, func, _optimizer(opt, current_ratio))
            fex = first(ret)
            args = Base.tail(ret)

            @show typeof(fex)
            @show typeof(args)

            # If the ratios switch sign, time to exit
            getratio(fex) <= getratio(opt) && break
            
            current_err = geterr(fex, opt)
            @info """
            Current Ratio: $(convert(Float64, current_ratio))
            Best Ratio: $(convert(Float64, best_ratio))

            Current Error: $(convert(Float64, current_err))
            Best Error: $(convert(Float64, best_err))
            """
            if current_err < best_err
                best_ratio = current_ratio
                best_err = current_err
                best_fex = fex
                best_args = args
            end
        end
        current_ratio = best_ratio
    end

    return (best_fex, best_args...)
end


function _factory(backend::nGraph.Backend{nGraph.CPU}, func, opt)
    # Unpack and compile the function
    fex = actualize(backend, func)
    #apply_affinity_heuristic!(fex.ex.ngraph_function)

    data = profile(fex)
    modeltype = opt(data, backend)

    # Some data structures for keeping track of modeling and optimization time.
    creation_times = Float64[]  
    optimization_times = Float64[]

    # Iterate until convergence
    while true
        # Optimize the function
        creation_time = @elapsed(frame = create_model(modeltype, data))
        optimization_time = @elapsed(optimize!(frame))
        fex, _metadata = configure!(fex, frame)

        push!(creation_times, creation_time)
        push!(optimization_times, optimization_time)

        # Deal with fragmentation
        if exceeds_limit(fex, modeltype)
            @info """
            Limit Exceeded
            Limit: $(maxlimit(modeltype))
            Actual: $(convert(Int, nGraph.get_temporary_pool_size(fex.ex.ngraph_function)))
            """

            modeltype = update(modeltype, frame.profile_data)

            # Update the flux executable
            fex = actualize(backend, func)
            #apply_affinity_heuristic!(fex.ex.ngraph_function)

            data = profile(fex)
        # Adjust ratio if outside of of the desired bounds
        else
            frame.profile_data = profile(fex)

            metadata = Dict(
                :metadata => _metadata,
                :creation_times => creation_times,
                :optimization_times => optimization_times,
            )

            return fex, frame, metadata
        end
    end
end

#####
##### GPU factory
#####

# GPU Memory in Bytes
const GPU_MAX_MEMORY = 11000000
const GPU_MEMORY_OVERHEAD = 500000

function _factory(backend::nGraph.Backend{nGraph.GPU}, func, opt::T) where {T <: AbstractOptimizer}
    # Get the function, arguments, and keyword arguments from the provided function
    f, args, kw = func()

    # add a callback that will populate a reference to a `ProfileData` type
    frame_ref = Ref{Frame}()
    limits_ref = Ref{Vector{Int}}() 

    #A callback that profiles the ngraph function
    function cb(f::nGraph.NFunction)
        # Do some minor editing the order of nodes in the graph to hopefully yield slightly
        # better memory characteristics
        #apply_affinity_heuristic!(f)

        data = profile(f, backend)

        # Initialize the node dram limits if needed
        if !isdefined(limits_ref, :x)
            # Get the limit from the optimizer
            # Find the input and output size of the function and subtract that from the 
            # limit along with a fudge factor so we can fit the model on the GPU
            io_size = sum(sizeof, input_tensors(f)) + sum(sizeof, output_tensors(f))
            limit = getlimit(opt)
            adjusted_limit = limit - io_size

            # LOL, like this is all we need to check
            @assert adjusted_limit > 0 
            @info "Adjusted Limit: $adjusted_limit"

            # Create a duplicate of the original optimizer with the adjusted limit
            opt_adjusted = _optimizer(opt, adjusted_limit)
            modeltype = opt_adjusted(data, backend)
            limits_ref[] = modeltype.dram_limits
        else
            modeltype = opt(data, backend)
            modeltype.dram_limits = limits_ref[]
        end

        #modeltype = asynchronous(limits_ref[], 12000, 12000, 12000, 12000)

        frame = create_model(modeltype, data)
        optimize!(frame)
        tensor_map = configure!(f, frame)

        frame_ref[] = frame
        return nothing
    end

    # Defrag callback - if a function needs defragging, throws a `GPUExit` exception to
    # avoid nGraph trying to allocate too much GPU memory
    function defrag(f::nGraph.NFunction)
        if exceeds_limit(f, frame_ref[].modeltype)
            # This is pretty ugly - sorry about that.
            modeltype = update(frame_ref[].modeltype, profile(f, backend))
            limits_ref[] = modeltype.dram_limits

            throw(GPUExit())
        end
    end

    # Compile the function to a ngraph executable
    local fex
    retry = true
    while retry
        retry = false

        # Setup callbacks
        #
        # If the function needs defragging, a `GPUExit` exception will be thrown and we
        # will have to try again.
        gpu_callbacks = GPUCallback()
        callback!(gpu_callbacks, cb)
        callback!(gpu_callbacks, defrag)

        try
            fex = nGraph.compile(
                backend, 
                f, 
                args...;
                callback = gpu_callbacks, 
                emit_timing = true, 
                kw...
            )
        catch e
            isa(e, GPUExit) || rethrow(e)
            retry = true
        end
    end

    return fex, frame_ref[]
end

